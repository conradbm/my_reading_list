%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/

%% Created for Tom Cook at 2017-08-24 09:48:29 -0500 


%% Saved with string encoding Unicode (UTF-8) 



@book{elliott:20132a,
	Author = {Elliott, Graham and Timmermann, Allan},
	Date-Added = {2017-08-24 14:48:10 +0000},
	Date-Modified = {2017-08-24 14:48:28 +0000},
	Publisher = {Elsevier},
	Title = {Handbook of economic forecasting},
	Url = {http://econpapers.repec.org/bookchap/eeeecofor/2.htm},
	Year = {2013}}

@inproceedings{szegedy2015going,
	Author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	Booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Pages = {1--9},
	Title = {Going deeper with convolutions},
	Url = {http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf},
	Year = {2015},
	Bdsk-Url-1 = {http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf}}

@book{gurney1997introduction,
	Author = {Gurney, Kevin},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Publisher = {CRC press},
	Title = {An introduction to neural networks},
	Url = {http://www.inf.ed.ac.uk/teaching/courses/nlu/reading/Gurney_et_al.pdf},
	Year = {1997},
	Bdsk-Url-1 = {http://www.inf.ed.ac.uk/teaching/courses/nlu/reading/Gurney_et_al.pdf}}

@manual{Stanford:2016aa,
	Author = {Stanford},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Organization = {Stanford},
	Title = {supervised learning tutorial},
	Url = {http://ufldl.stanford.edu/tutorial/},
	Year = {2016},
	Bdsk-Url-1 = {http://ufldl.stanford.edu/tutorial/}}

@manual{:2016aa,
	Author = {Fjodor van Veen},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Lastchecked = {09/30/2016},
	Month = {Sept},
	Title = {A mostly complete chart of neural networks},
	Url = {http://www.asimovinstitute.org/neural-network-zoo/},
	Year = {2016},
	Bdsk-Url-1 = {http://www.asimovinstitute.org/neural-network-zoo/}}

@article{Bourlard:1988aa,
	Annote = {foundational article on auto-encoders},
	Author = {Bourlard, Herv{\'e} and Kamp, Yves},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Journal = {Biological cybernetics},
	Number = {4-5},
	Pages = {291--294},
	Publisher = {Springer},
	Title = {Auto-association by multilayer perceptrons and singular value decomposition},
	Volume = {59},
	Year = {1988}}

@inproceedings{Zeiler:2010,
	Author = {Zeiler, Matthew D and Krishnan, Dilip and Taylor, Graham W and Fergus, Rob},
	Booktitle = {Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Organization = {IEEE},
	Pages = {2528--2535},
	Title = {Deconvolutional networks},
	Year = {2010}}

@inproceedings{zeiler:20102a,
	Author = {Zeiler, Matthew D and Krishnan, Dilip and Taylor, Graham W and Fergus, Rob},
	Booktitle = {Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Organization = {IEEE},
	Pages = {2528--2535},
	Title = {Deconvolutional networks},
	Year = {2010}}

@inproceedings{yosinski:20142a,
	Author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
	Booktitle = {Advances in neural information processing systems},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Pages = {3320--3328},
	Title = {How transferable are features in deep neural networks?},
	Url = {https://arxiv.org/abs/1411.1792},
	Year = {2014},
	Bdsk-Url-1 = {https://arxiv.org/abs/1411.1792}}

@article{dalto:2a,
	Author = {Dalto, Mladen},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Journal = {Rn ($\Theta$1)},
	Number = {2},
	Title = {Deep neural networks for time series prediction with applications in ultra-short-term wind forecasting},
	Url = {https://www.fer.unizg.hr/_download/repository/KDI-Djalto.pdf},
	Volume = {1},
	Bdsk-Url-1 = {https://www.fer.unizg.hr/_download/repository/KDI-Djalto.pdf}}

@inproceedings{li:20162a,
	Author = {Fei-Fei Li AND Andrej Karpathy AND Justin Johnson},
	Booktitle = {CS231n: Convolutional Neural Networks for Visual Recognition},
	Crossref = {li:20162a},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Title = {CS231n: Convolutional Neural Networks for Visual Recognition},
	Url = {http://cs231n.stanford.edu/},
	Year = {2016},
	Bdsk-Url-1 = {http://cs231n.stanford.edu/}}

@inproceedings{li:20162b,
	Crossref = {li:20162a},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Url = {https://www.youtube.com/playlist?list=PL16j5WbGpaM0_Tj8CRmurZ8Kk1gEBc7fg},
	Year = {2016},
	Bdsk-Url-1 = {https://www.youtube.com/playlist?list=PL16j5WbGpaM0_Tj8CRmurZ8Kk1gEBc7fg}}

@article{foerster2016learning,
	Author = {Foerster, Jakob N and Assael, Yannis M and de Freitas, Nando and Whiteson, Shimon},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Journal = {arXiv preprint arXiv:1605.06676},
	Title = {Learning to Communicate with Deep Multi-Agent Reinforcement Learning},
	Url = {https://arxiv.org/abs/1605.06676v2},
	Year = {2016},
	Bdsk-Url-1 = {https://arxiv.org/abs/1605.06676v2}}

@article{mnih2016strategic,
	Author = {Mnih, Volodymyr and Agapiou, John and Osindero, Simon and Graves, Alex and Vinyals, Oriol and Kavukcuoglu, Koray and others},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Journal = {arXiv preprint arXiv:1606.04695},
	Title = {Strategic Attentive Writer for Learning Macro-Actions},
	Url = {https://arxiv.org/abs/1606.04695},
	Year = {2016},
	Bdsk-Url-1 = {https://arxiv.org/abs/1606.04695}}

@article{foerster:20162a,
	Abstract = {We consider the problem of multiple agents sensing and acting in environments with the goal of maximising their shared utility. In these environments, agents must learn communication protocols in order to share information that is needed to solve the tasks. By embracing deep neural networks, we are able to demonstrate end-to-end learning of protocols in complex environments inspired by communication riddles and multi-agent computer vision problems with partial observability. We propose two approaches for learning in these domains: Reinforced Inter-Agent Learning (RIAL) and Differentiable Inter-Agent Learning (DIAL). The former uses deep Q-learning, while the latter exploits the fact that, during learning, agents can backpropagate error derivatives through (noisy) communication channels. Hence, this approach uses centralised learning but decentralised execution. Our experiments introduce new environments for studying the learning of communication protocols and present a set of engineering innovations that are essential for success in these domains.},
	Author = {Jakob N. Foerster and Yannis M. Assael and Nando de Freitas and Shimon Whiteson},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Eprint = {1605.06676},
	Month = {05},
	Title = {Learning to Communicate with Deep Multi-Agent Reinforcement Learning},
	Url = {https://arxiv.org/abs/1605.06676},
	Year = {2016},
	Bdsk-Url-1 = {https://arxiv.org/abs/1605.06676}}

@article{alexander:20162a,
	Abstract = {We present a novel deep recurrent neural network architecture that learns to build implicit plans in an end-to-end manner by purely interacting with an environment in reinforcement learning setting. The network builds an internal plan, which is continuously updated upon observation of the next input from the environment. It can also partition this internal representation into contiguous sub- sequences by learning for how long the plan can be committed to - i.e. followed without re-planing. Combining these properties, the proposed model, dubbed STRategic Attentive Writer (STRAW) can learn high-level, temporally abstracted macro- actions of varying lengths that are solely learnt from data without any prior information. These macro-actions enable both structured exploration and economic computation. We experimentally demonstrate that STRAW delivers strong improvements on several ATARI games by employing temporally extended planning strategies (e.g. Ms. Pacman and Frostbite). It is at the same time a general algorithm that can be applied on any sequence data. To that end, we also show that when trained on text prediction task, STRAW naturally predicts frequent n-grams (instead of macro-actions), demonstrating the generality of the approach.},
	Author = {Alexander and Vezhnevets and Volodymyr Mnih and John Agapiou and Simon Osindero and Alex Graves and Oriol Vinyals and Koray Kavukcuoglu},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Eprint = {1606.04695},
	Month = {06},
	Title = {Strategic Attentive Writer for Learning Macro-Actions},
	Url = {https://arxiv.org/abs/1606.04695},
	Year = {2016},
	Bdsk-Url-1 = {https://arxiv.org/abs/1606.04695}}

@article{fraccaro:20162a,
	Abstract = {How can we efficiently propagate uncertainty in a latent state representation with recurrent neural networks? This paper introduces stochastic recurrent neural networks which glue a deterministic recurrent neural network and a state space model together to form a stochastic and sequential neural generative model. The clear separation of deterministic and stochastic layers allows a structured variational inference network to track the factorization of the model's posterior distribution. By retaining both the nonlinear recursive structure of a recurrent neural network and averaging over the uncertainty in a latent path, like a state space model, we improve the state of the art results on the Blizzard and TIMIT speech modeling data sets by a large margin, while achieving comparable performances to competing methods on polyphonic music modeling.},
	Author = {Marco Fraccaro and S{\o}ren Kaae S{\o}nderby and Ulrich Paquet and Ole Winther},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Eprint = {1605.07571},
	Month = {05},
	Title = {Sequential Neural Models with Stochastic Layers},
	Url = {https://arxiv.org/abs/1605.07571},
	Year = {2016},
	Bdsk-Url-1 = {https://arxiv.org/abs/1605.07571}}

@article{osband:20162a,
	Abstract = {Efficient exploration in complex environments remains a major challenge for reinforcement learning. We propose bootstrapped DQN, a simple algorithm that explores in a computationally and statistically efficient manner through use of randomized value functions. Unlike dithering strategies such as epsilon-greedy exploration, bootstrapped DQN carries out temporally-extended (or deep) exploration; this can lead to exponentially faster learning. We demonstrate these benefits in complex stochastic MDPs and in the large-scale Arcade Learning Environment. Bootstrapped DQN substantially improves learning times and performance across most Atari games.},
	Author = {Ian Osband and Charles Blundell and Alexander Pritzel and Benjamin Van Roy},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Eprint = {1602.04621},
	Month = {02},
	Title = {Deep Exploration via Bootstrapped DQN},
	Url = {https://arxiv.org/abs/1602.04621},
	Year = {2016},
	Bdsk-Url-1 = {https://arxiv.org/abs/1602.04621}}

@article{jaderberg:20162a,
	Abstract = {Training directed neural networks typically requires forward-propagating data through a computation graph, followed by backpropagating error signal, to produce weight updates. All layers, or more generally, modules, of the network are therefore locked, in the sense that they must wait for the remainder of the network to execute forwards and propagate error backwards before they can be updated. In this work we break this constraint by decoupling modules by introducing a model of the future computation of the network graph. These models predict what the result of the modelled subgraph will produce using only local information. In particular we focus on modelling error gradients: by using the modelled synthetic gradient in place of true backpropagated error gradients we decouple subgraphs, and can update them independently and asynchronously i.e. we realise decoupled neural interfaces. We show results for feed-forward models, where every layer is trained asynchronously, recurrent neural networks (RNNs) where predicting one's future gradient extends the time over which the RNN can effectively model, and also a hierarchical RNN system with ticking at different timescales. Finally, we demonstrate that in addition to predicting gradients, the same framework can be used to predict inputs, resulting in models which are decoupled in both the forward and backwards pass -- amounting to independent networks which co-learn such that they can be composed into a single functioning corporation.},
	Author = {Max Jaderberg and Wojciech Marian Czarnecki and Simon Osindero and Oriol Vinyals and Alex Graves and Koray Kavukcuoglu},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Eprint = {1608.05343},
	Month = {08},
	Title = {Decoupled Neural Interfaces using Synthetic Gradients},
	Url = {https://arxiv.org/abs/1608.05343},
	Year = {2016},
	Bdsk-Url-1 = {https://arxiv.org/abs/1608.05343}}

@article{rusu:20162a,
	Abstract = {Learning to solve complex sequences of tasks--while both leveraging transfer and avoiding catastrophic forgetting--remains a key obstacle to achieving human-level intelligence. The progressive networks approach represents a step forward in this direction: they are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features. We evaluate this architecture extensively on a wide variety of reinforcement learning tasks (Atari and 3D maze games), and show that it outperforms common baselines based on pretraining and finetuning. Using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.},
	Author = {Andrei A. Rusu and Neil C. Rabinowitz and Guillaume Desjardins and Hubert Soyer and James Kirkpatrick and Koray Kavukcuoglu and Razvan Pascanu and Raia Hadsell},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Eprint = {1606.04671},
	Month = {06},
	Title = {Progressive Neural Networks},
	Url = {https://arxiv.org/abs/1606.04671},
	Year = {2016},
	Bdsk-Url-1 = {https://arxiv.org/abs/1606.04671}}

@article{rezende:20162a,
	Abstract = {Humans have an impressive ability to reason about new concepts and experiences from just a single example. In particular, humans have an ability for one-shot generalization: an ability to encounter a new concept, understand its structure, and then be able to generate compelling alternative variations of the concept. We develop machine learning systems with this important capacity by developing new deep generative models, models that combine the representational power of deep learning with the inferential power of Bayesian reasoning. We develop a class of sequential generative models that are built on the principles of feedback and attention. These two characteristics lead to generative models that are among the state-of-the art in density estimation and image generation. We demonstrate the one-shot generalization ability of our models using three tasks: unconditional sampling, generating new exemplars of a given concept, and generating new exemplars of a family of concepts. In all cases our models are able to generate compelling and diverse samples---having seen new examples just once---providing an important class of general-purpose models for one-shot machine learning.},
	Author = {Danilo Jimenez Rezende and Shakir Mohamed and Ivo Danihelka and Karol Gregor and Daan Wierstra},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Eprint = {1603.05106},
	Month = {03},
	Title = {One-Shot Generalization in Deep Generative Models},
	Url = {https://arxiv.org/abs/1603.05106},
	Year = {2016},
	Bdsk-Url-1 = {https://arxiv.org/abs/1603.05106}}

@article{wu:20162a,
	Author = {Yonghui Wu and Mike Schuster and Zhifeng Chen and Quoc V. Le and Mohammad Norouzi and Wolfgang Macherey and Maxim Krikun and Yuan Cao and Qin Gao and Klaus Macherey and Jeff Klingner and Apurva Shah and Melvin Johnson and Xiaobing Liu and {\L}ukasz Kaiser and Stephan Gouws and Yoshikiyo Kato and Taku Kudo and Hideto Kazawa and Keith Stevens and George Kurian and Nishant Patil and Wei Wang and Cliff Young and Jason Smith and Jason Riesa and Alex Rudnick and Oriol Vinyals and Greg Corrado and Macduff Hughes and Jeffrey Dean},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Eprint = {1609.08144},
	Month = {09},
	Title = {Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation},
	Url = {https://arxiv.org/abs/1609.08144},
	Year = {2016},
	Bdsk-Url-1 = {https://arxiv.org/abs/1609.08144}}

@article{andrychowicz:20162a,
	Abstract = {The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.},
	Author = {Marcin Andrychowicz and Misha Denil and Sergio Gomez and Matthew W. Hoffman and David Pfau and Tom Schaul and Nando de Freitas},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Eprint = {1606.04474},
	Month = {06},
	Title = {Learning to learn by gradient descent by gradient descent},
	Url = {https://arxiv.org/abs/1606.04474},
	Year = {2016},
	Bdsk-Url-1 = {https://arxiv.org/abs/1606.04474}}

@article{szegedy:20162a,
	Abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge},
	Author = {Christian Szegedy and Sergey Ioffe and Vincent Vanhoucke and Alex Alemi},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Eprint = {1602.07261},
	Month = {02},
	Title = {Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning},
	Url = {https://arxiv.org/abs/1602.07261},
	Year = {2016},
	Bdsk-Url-1 = {https://arxiv.org/abs/1602.07261}}

@article{simonyan:20142a,
	Abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	Author = {Karen Simonyan and Andrew Zisserman},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Eprint = {1409.1556},
	Month = {09},
	Title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
	Url = {https://arxiv.org/abs/1409.1556},
	Year = {2014},
	Bdsk-Url-1 = {https://arxiv.org/abs/1409.1556}}

@article{he:20152a,
	Abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
	Author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Eprint = {1512.03385},
	Month = {12},
	Title = {Deep Residual Learning for Image Recognition},
	Url = {https://arxiv.org/abs/1512.03385},
	Year = {2015},
	Bdsk-Url-1 = {https://arxiv.org/abs/1512.03385}}

@article{andor:20162a,
	Abstract = {We introduce a globally normalized transition-based neural network model that achieves state-of-the-art part-of-speech tagging, dependency parsing and sentence compression results. Our model is a simple feed-forward neural network that operates on a task-specific transition system, yet achieves comparable or better accuracies than recurrent models. We discuss the importance of global as opposed to local normalization: a key insight is that the label bias problem implies that globally normalized models can be strictly more expressive than locally normalized models.},
	Author = {Daniel Andor and Chris Alberti and David Weiss and Aliaksei Severyn and Alessandro Presta and Kuzman Ganchev and Slav Petrov and Michael Collins},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Eprint = {1603.06042},
	Month = {03},
	Title = {Globally Normalized Transition-Based Neural Networks},
	Url = {https://arxiv.org/abs/1603.06042},
	Year = {2016},
	Bdsk-Url-1 = {https://arxiv.org/abs/1603.06042}}

@article{landefeld2008taking,
	Author = {Landefeld, Steven J and Seskin, Eugene P and Fraumeni, Barbara M},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Doi = {10.1257/jep.22.2.193},
	Journal = {The Journal of Economic Perspectives},
	Number = {2},
	Pages = {193--193},
	Publisher = {American Economic Association},
	Title = {Taking the pulse of the economy: Measuring GDP},
	Url = {https://www.aeaweb.org/articles?id=10.1257/jep.22.2.193},
	Volume = {22},
	Year = {2008},
	Bdsk-Url-1 = {https://www.aeaweb.org/articles?id=10.1257/jep.22.2.193}}

@article{graves2016hybrid,
	Author = {Graves, Alex and Wayne, Greg and Reynolds, Malcolm and Harley, Tim and Danihelka, Ivo and Grabska-Barwi{\'n}ska, Agnieszka and Colmenarejo, Sergio G{\'o}mez and Grefenstette, Edward and Ramalho, Tiago and Agapiou, John and others},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Journal = {Nature},
	Publisher = {Nature Research},
	Title = {Hybrid computing using a neural network with dynamic external memory},
	Year = {2016}}

@article{graves:20142b,
	Abstract = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
	Author = {Alex Graves and Greg Wayne and Ivo Danihelka},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Eprint = {1410.5401},
	Month = {10},
	Title = {Neural Turing Machines},
	Url = {https://arxiv.org/abs/1410.5401},
	Year = {2014},
	Bdsk-Url-1 = {https://arxiv.org/abs/1410.5401}}

@article{chung:20142a,
	Abstract = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
	Author = {Junyoung Chung and Caglar Gulcehre and KyungHyun Cho and Yoshua Bengio},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Eprint = {1412.3555},
	Month = {12},
	Title = {Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling},
	Url = {https://arxiv.org/abs/1412.3555},
	Year = {2014},
	Bdsk-Url-1 = {https://arxiv.org/abs/1412.3555}}

@article{huang:20162a,
	Abstract = {Very deep convolutional networks with hundreds of layers have led to significant reductions in error on competitive benchmarks. Although the unmatched expressiveness of the many layers can be highly desirable at test time, training very deep networks comes with its own set of challenges. The gradients can vanish, the forward flow often diminishes, and the training time can be painfully slow. To address these problems, we propose stochastic depth, a training procedure that enables the seemingly contradictory setup to train short networks and use deep networks at test time. We start with very deep networks but during training, for each mini-batch, randomly drop a subset of layers and bypass them with the identity function. This simple approach complements the recent success of residual networks. It reduces training time substantially and improves the test error significantly on almost all data sets that we used for evaluation. With stochastic depth we can increase the depth of residual networks even beyond 1200 layers and still yield meaningful improvements in test error (4.91% on CIFAR-10).},
	Author = {Gao Huang and Yu Sun and Zhuang Liu and Daniel Sedra and Kilian Weinberger},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Eprint = {1603.09382},
	Month = {03},
	Title = {Deep Networks with Stochastic Depth},
	Url = {https://arxiv.org/abs/1603.09382},
	Year = {2016},
	Bdsk-Url-1 = {https://arxiv.org/abs/1603.09382}}

@article{heaton:20162a,
	Abstract = {We explore the use of deep learning hierarchical models for problems in financial prediction and classification. Financial prediction problems -- such as those presented in designing and pricing securities, constructing portfolios, and risk management -- often involve large data sets with complex data interactions that currently are difficult or impossible to specify in a full economic model. Applying deep learning methods to these problems can produce more useful results than standard methods in finance. In particular, deep learning can detect and exploit interactions in the data that are, at least currently, invisible to any existing financial economic theory.},
	Author = {J. B. Heaton and N. G. Polson and J. H. Witte},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Eprint = {1602.06561},
	Month = {02},
	Title = {Deep Learning in Finance},
	Url = {https://arxiv.org/abs/1602.06561},
	Year = {2016},
	Bdsk-Url-1 = {https://arxiv.org/abs/1602.06561}}

@article{Niaki2013,
	Abstract = {The main objective of this research is to forecast the daily direction of Standard {\&amp;} Poor's 500 (S{\&amp;}P 500) index using an artificial neural network (ANN). In order to select the most influential features (factors) of the proposed ANN that affect the daily direction of S{\&amp;}P 500 (the response), design of experiments are conducted to determine the statistically significant factors among 27 potential financial and economical variables along with a feature defined as the number of nodes of the ANN. The results of employing the proposed methodology show that the ANN that uses the most influential features is able to forecast the daily direction of S{\&amp;}P 500 significantly better than the traditional logit model. Furthermore, experimental results of employing the proposed ANN on the trades in a test period indicate that ANN could significantly improve the trading profit as compared with the buy-and-hold strategy.},
	Author = {Niaki, Seyed Taghi Akhavan and Hoseinzade, Saeid},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Doi = {10.1186/2251-712X-9-1},
	Issn = {2251-712X},
	Journal = {Journal of Industrial Engineering International},
	Number = {1},
	Pages = {1},
	Title = {Forecasting S{\&amp;}P 500 index using artificial neural networks and design of experiments},
	Url = {http://dx.doi.org/10.1186/2251-712X-9-1},
	Volume = {9},
	Year = {2013},
	Bdsk-Url-1 = {http://dx.doi.org/10.1186/2251-712X-9-1}}

@inproceedings{deng:20142a,
	Author = {Li Deng and John C. Platt},
	Booktitle = {INTERSPEECH},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Title = {Ensemble deep learning for speech recognition},
	Year = {2014}}

@article{guclu:20142a,
	Author = {Umut G\"{u}\c{c}l\"{u} and Marcel A. J. van Gerven},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Eprint = {1411.6422},
	Month = {11},
	Title = {Deep Neural Networks Reveal a Gradient in the Complexity of Neural Representations across the Brain's Ventral Visual Pathway},
	Url = {https://arxiv.org/abs/1411.6422},
	Year = {2014},
	Bdsk-Url-1 = {https://arxiv.org/abs/1411.6422}}

@url{veen:20162a,
	Author = {Fjodor van Veen},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Url = {http://www.asimovinstitute.org/neural-network-zoo/},
	Year = {2016},
	Bdsk-Url-1 = {http://www.asimovinstitute.org/neural-network-zoo/}}

@book{sutton:19982a,
	Author = {Sutton, Richard S and Barto, Andrew G},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Number = {1},
	Publisher = {MIT press Cambridge},
	Title = {Reinforcement learning: An introduction},
	Url = {https://webdocs.cs.ualberta.ca/~sutton/book/the-book.html},
	Volume = {1},
	Year = {1998},
	Bdsk-Url-1 = {https://webdocs.cs.ualberta.ca/~sutton/book/the-book.html}}

@webpage{silver:2a,
	Author = {David Silver},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Lastchecked = {2016-12-16},
	Title = {Reinforcement Learning Lectures},
	Url = {https://youtu.be/2pWv7GOvuf0},
	Bdsk-Url-1 = {https://youtu.be/2pWv7GOvuf0}}

@article{sukhbaatar:20162a,
	Abstract = {Many tasks in AI require the collaboration of multiple agents. Typically, the communication protocol between agents is manually specified and not altered during training. In this paper we explore a simple neural model, called CommNet, that uses continuous communication for fully cooperative tasks. The model consists of multiple agents and the communication between them is learned alongside their policy. We apply this model to a diverse set of tasks, demonstrating the ability of the agents to learn to communicate amongst themselves, yielding improved performance over non-communicative agents and baselines. In some cases, it is possible to interpret the language devised by the agents, revealing simple but effective strategies for solving the task at hand.},
	Author = {Sainbayar Sukhbaatar and Arthur Szlam and Rob Fergus},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Eprint = {1605.07736},
	Month = {05},
	Title = {Learning Multiagent Communication with Backpropagation},
	Url = {https://arxiv.org/abs/1605.07736},
	Year = {2016},
	Bdsk-Url-1 = {https://arxiv.org/abs/1605.07736}}

@article{heinrich:20162a,
	Abstract = {Many real-world applications can be described as large-scale games of imperfect information. To deal with these challenging domains, prior work has focused on computing Nash equilibria in a handcrafted abstraction of the domain. In this paper we introduce the first scalable end-to-end approach to learning approximate Nash equilibria without prior domain knowledge. Our method combines fictitious self-play with deep reinforcement learning. When applied to Leduc poker, Neural Fictitious Self-Play (NFSP) approached a Nash equilibrium, whereas common reinforcement learning methods diverged. In Limit Texas Holdem, a poker game of real-world scale, NFSP learnt a strategy that approached the performance of state-of-the-art, superhuman algorithms based on significant domain expertise.},
	Author = {Johannes Heinrich and David Silver},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Eprint = {1603.01121},
	Month = {03},
	Title = {Deep Reinforcement Learning from Self-Play in Imperfect-Information Games},
	Url = {https://arxiv.org/abs/1603.01121},
	Year = {2016},
	Bdsk-Url-1 = {https://arxiv.org/abs/1603.01121}}

@inproceedings{hartford:20162a,
	Author = {Hartford, Jason S and Wright, James R and Leyton-Brown, Kevin},
	Booktitle = {Advances in Neural Information Processing Systems},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Pages = {2424--2432},
	Title = {Deep learning for predicting human strategic behavior},
	Url = {http://www.cs.ubc.ca/~jasonhar/GameNet-NIPS-2016.pdf},
	Year = {2016},
	Bdsk-Url-1 = {http://www.cs.ubc.ca/~jasonhar/GameNet-NIPS-2016.pdf}}

@article{hartford:20162b,
	Abstract = {We are in the middle of a remarkable rise in the use and capability of artificial intelligence. Much of this growth has been fueled by the success of deep learning architectures: models that map from observables to outputs via multiple layers of latent representations. These deep learning algorithms are effective tools for unstructured prediction, and they can be combined in AI systems to solve complex automated reasoning problems. This paper provides a recipe for combining ML algorithms to solve for causal effects in the presence of instrumental variables -- sources of treatment randomization that are conditionally independent from the response. We show that a flexible IV specification resolves into two prediction tasks that can be solved with deep neural nets: a first-stage network for treatment prediction and a second-stage network whose loss function involves integration over the conditional treatment distribution. This Deep IV framework imposes some specific structure on the stochastic gradient descent routine used for training, but it is general enough that we can take advantage of off-the-shelf ML capabilities and avoid extensive algorithm customization. We outline how to obtain out-of-sample causal validation in order to avoid over-fit. We also introduce schemes for both Bayesian and frequentist inference: the former via a novel adaptation of dropout training, and the latter via a data splitting routine.},
	Author = {Jason Hartford and Greg Lewis and Kevin Leyton-Brown and Matt Taddy},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Eprint = {1612.09596},
	Month = {12},
	Title = {Counterfactual Prediction with Deep Instrumental Variables Networks},
	Url = {https://arxiv.org/abs/1612.09596},
	Year = {2016},
	Bdsk-Url-1 = {https://arxiv.org/abs/1612.09596}}

@techreport{RePEc:fip:fedawp:2014-07,
	Abstract = {This paper documents GDPNow, a \&quot;nowcasting\&quot; model for gross domestic product (GDP) growth that synthesizes the \&quot;bridge equation\&quot; approach relating GDP subcomponents to monthly source data with the factor model approach used by Giannone, Reichlin, and Small (2008). The GDPNow model forecasts GDP growth by aggregating 13 subcomponents that make up GDP with the chain-weighting methodology used by the U.S. Bureau of Economic Analysis. Using current vintage data, out-of-sample GDPNow model forecasts are found to be more accurate than a number of statistical benchmarks since 2000. Using real-time data since the second-half of 2011, GDPNow model forecasts are found to be only slightly inferior to consensus near-term GDP forecasts from Blue Chip Economic Indicators. The forecast error variance of GDP growth for each of the GDPNow model, Blue Chip, and the Federal Reserve staff's Green Book is decomposed as the sum of the forecast error covariances for the contributions to growth of the subcomponents of GDP. The decompositions show that \&quot;net exports\&quot; and \&quot;change in private inventories\&quot; are particularly difficult subcomponents to nowcast.},
	Author = {Higgins, Patrick C.},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Institution = {Federal Reserve Bank of Atlanta},
	Keywords = {nowcasting; forecasting; macroeconometric forecasting},
	Month = Jul,
	Number = {2014-7},
	Title = {{GDPNow: A Model for GDP ``Nowcasting''}},
	Type = {FRB Atlanta Working Paper},
	Url = {https://ideas.repec.org/p/fip/fedawp/2014-07.html},
	Year = 2014,
	Bdsk-Url-1 = {https://ideas.repec.org/p/fip/fedawp/2014-07.html}}

@article{klein:19502a,
	Author = {Klein, Lawrence Robert},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Publisher = {Wiley},
	Title = {Economic fluctuations in the United States, 1921-1941},
	Year = {1950}}

@techreport{diebold:19972a,
	Author = {Diebold, Francis X},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Institution = {National Bureau of Economic Research},
	Title = {The past, present, and future of macroeconomic forecasting},
	Url = {http://www.nber.org/papers/w6290.pdf},
	Year = {1997},
	Bdsk-Url-1 = {http://www.nber.org/papers/w6290.pdf}}

@periodical{herzon:20152a,
	Author = {Ben Herzon},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Journal = {Macroeconomic Advisers' Macro Focus},
	Title = {The Atlanta Fed's GDPNow and MA GDP Tracking: An Update},
	Volume = {10:21},
	Year = {2015}}

@article{kliesen:20162a,
	Author = {Kevin L. Kliesen and Michael W. McCracken},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Journal = {The Regional Economist},
	Title = {Tracking the US Economy with Nowcasts},
	Url = {https://www.stlouisfed.org/publications/regional-economist/april-2016/tracking-the-us-economy-with-nowcasts},
	Urldate = {01-03-2017},
	Year = {2016},
	Bdsk-Url-1 = {https://www.stlouisfed.org/publications/regional-economist/april-2016/tracking-the-us-economy-with-nowcasts}}

@techreport{sinclair2016fed,
	Author = {Sinclair, Tara and Tien, Pao-Lin and Gamber, Edward and others},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Title = {Do Fed Forecast Errors Matter?},
	Url = {https://ideas.repec.org/p/gwi/wpaper/2016-14.html},
	Year = {2016},
	Bdsk-Url-1 = {https://ideas.repec.org/p/gwi/wpaper/2016-14.html}}

@article{atan:20162a,
	Abstract = {This paper proposes a novel approach for constructing effective personalized policies when the observed data lacks counter-factual information, is biased and possesses many features. The approach is applicable in a wide variety of settings from healthcare to advertising to education to finance. These settings have in common that the decision maker can observe, for each previous instance, an array of features of the instance, the action taken in that instance, and the reward realized -- but not the rewards of actions that were not taken: the counterfactual information. Learning in such settings is made even more difficult because the observed data is typically biased by the existing policy (that generated the data) and because the array of features that might affect the reward in a particular instance -- and hence should be taken into account in deciding on an action in each particular instance -- is often vast. The approach presented here estimates propensity scores for the observed data, infers counterfactuals, identifies a (relatively small) number of features that are (most) relevant for each possible action and instance, and prescribes a policy to be followed. Comparison of the proposed algorithm against the state-of-art algorithm on actual datasets demonstrates that the proposed algorithm achieves a significant improvement in performance.},
	Author = {Onur Atan and William R. Zame and Qiaojun Feng and Mihaela van der Schaar},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Eprint = {1612.08082},
	Month = {12},
	Title = {Constructing Effective Personalized Policies Using Counterfactual Inference from Biased Data Sets with Many Features},
	Url = {https://arxiv.org/abs/1612.08082},
	Year = {2016},
	Bdsk-Url-1 = {https://arxiv.org/abs/1612.08082}}

@article{cui:20162a,
	Author = {Zhicheng Cui and Wenlin Chen and Yixin Chen},
	Bibsource = {dblp computer science bibliography, http://dblp.org},
	Biburl = {http://dblp.uni-trier.de/rec/bib/journals/corr/CuiCC16},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Journal = {CoRR},
	Timestamp = {Sat, 02 Apr 2016 11:49:48 +0200},
	Title = {Multi-Scale Convolutional Neural Networks for Time Series Classification},
	Url = {http://arxiv.org/abs/1603.06995},
	Volume = {abs/1603.06995},
	Year = {2016},
	Bdsk-Url-1 = {http://arxiv.org/abs/1603.06995}}

@article{abdel2014convolutional,
	Author = {Abdel-Hamid, Ossama and Mohamed, Abdel-Rahman and Jiang, Hui and Deng, Li and Penn, Gerald and Yu, Dong},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Journal = {IEEE/ACM Transactions on audio, speech, and language processing},
	Number = {10},
	Pages = {1533--1545},
	Publisher = {IEEE},
	Title = {Convolutional neural networks for speech recognition},
	Volume = {22},
	Year = {2014}}

@article{wang:20152a,
	Abstract = {Inspired by recent successes of deep learning in computer vision, we propose a novel framework for encoding time series as different types of images, namely, Gramian Angular Summation/Difference Fields (GASF/GADF) and Markov Transition Fields (MTF). This enables the use of techniques from computer vision for time series classification and imputation. We used Tiled Convolutional Neural Networks (tiled CNNs) on 20 standard datasets to learn high-level features from the individual and compound GASF-GADF-MTF images. Our approaches achieve highly competitive results when compared to nine of the current best time series classification approaches. Inspired by the bijection property of GASF on 0/1 rescaled data, we train Denoised Auto-encoders (DA) on the GASF images of four standard and one synthesized compound dataset. The imputation MSE on test data is reduced by 12.18%-48.02% when compared to using the raw data. An analysis of the features and weights learned via tiled CNNs and DAs explains why the approaches work.},
	Author = {Zhiguang Wang and Tim Oates},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Eprint = {1506.00327},
	Month = {06},
	Title = {Imaging Time-Series to Improve Classification and Imputation},
	Url = {https://arxiv.org/abs/1506.00327},
	Year = {2015},
	Bdsk-Url-1 = {https://arxiv.org/abs/1506.00327}}

@article{liu:20162a,
	Abstract = {Time series is attracting more attention across statistics, machine learning and pattern recognition as it appears widely in both industry and academia, but few advances have been achieved in effective time series visualization due to its temporal dimensionality and complex dynamics. Inspired by recent effort on using network metrics to characterize time series for classification, we present an approach to visualize time series as complex networks based on the first order Markov process in its temporal ordering. In contrast to the classical bar charts, line plots and other statistics based graph, our approach delivers more intuitive visualization that better preserves both the temporal dependency and frequency structures. It provides a natural inverse operation to map the graph back to time series, making it possible to use graph statistics to characterize time series for better visual exploration and statistical analysis. Our experimental results suggest the effectiveness on various tasks such as system identification, pattern mining and classification on both synthetic and the real time series data.},
	Author = {Lu Liu and Zhiguang Wang},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Eprint = {1610.07273},
	Month = {10},
	Title = {Encoding Temporal Markov Dynamics in Graph for Time Series Visualization},
	Url = {https://arxiv.org/abs/1610.07273},
	Year = {2016},
	Bdsk-Url-1 = {https://arxiv.org/abs/1610.07273}}

@inproceedings{ye:20092a,
	Author = {Ye, Lexiang and Keogh, Eamonn},
	Booktitle = {Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Organization = {ACM},
	Pages = {947--956},
	Title = {Time series shapelets: a new primitive for data mining},
	Url = {http://alumni.cs.ucr.edu/~lexiangy/Shapelet/kdd2009shapelet.pdf},
	Year = {2009},
	Bdsk-Url-1 = {http://alumni.cs.ucr.edu/~lexiangy/Shapelet/kdd2009shapelet.pdf}}

@misc{nouri:20142a,
	Author = {Daniel Nouri},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Month = {7},
	Title = {Using deep learning to listen for whales},
	Url = {http://danielnouri.org/notes/2014/01/10/using-deep-learning-to-listen-for-whales/},
	Year = {2014},
	Bdsk-Url-1 = {http://danielnouri.org/notes/2014/01/10/using-deep-learning-to-listen-for-whales/}}

@article{mccracken2015fred,
	Author = {McCracken, Michael W and Ng, Serena},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Journal = {Journal of Business \& Economic Statistics},
	Publisher = {Taylor \& Francis},
	Title = {FRED-MD: A monthly database for macroeconomic research},
	Url = {https://research.stlouisfed.org/econ/mccracken/fred-databases/},
	Year = {2015},
	Bdsk-Url-1 = {https://research.stlouisfed.org/econ/mccracken/fred-databases/}}

@manual{:20162a,
	Address = {Philadelphia, PA},
	Author = {Survey of Professional Forecasters},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Month = {may},
	Organization = {Federal Reserve Bank of Philadelphia},
	Title = {Documentation},
	Url = {https://www.philadelphiafed.org/-/media/research-and-data/real-time-center/survey-of-professional-forecasters/spf-documentation.pdf?la=en},
	Year = {2016},
	Bdsk-Url-1 = {https://www.philadelphiafed.org/-/media/research-and-data/real-time-center/survey-of-professional-forecasters/spf-documentation.pdf?la=en}}

@article{walsh:19952a,
	Author = {Walsh, Carl E},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Journal = {The American Economic Review},
	Pages = {150--167},
	Publisher = {JSTOR},
	Title = {Optimal contracts for central bankers},
	Year = {1995}}

@article{barnichon:20122a,
	Author = {Barnichon, Regis and Nekarda, Christopher J},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Journal = {Brookings Papers on Economic Activity},
	Number = {2},
	Pages = {83--131},
	Publisher = {Brookings Institution Press},
	Title = {The ins and outs of forecasting unemployment: Using labor force flows to forecast the labor market},
	Volume = {2012},
	Year = {2012}}

@article{tasci2015forecasting,
	Author = {Tasci, Murat and Treanor, Caitlin and others},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Journal = {Economic Commentary},
	Number = {Nov},
	Publisher = {Federal Reserve Bank of Cleveland},
	Title = {Forecasting Unemployment in Real Time during the Great Recession: An Elusive Task},
	Year = {2015}}

@techreport{diebold:19972b,
	Author = {Diebold, Francis X},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Institution = {National Bureau of Economic Research},
	Title = {The past, present, and future of macroeconomic forecasting},
	Url = {http://www.nber.org/papers/w6290.pdf},
	Year = {1997},
	Bdsk-Url-1 = {http://www.nber.org/papers/w6290.pdf}}

@book{hendry:20122a,
	Author = {Hendry, David F and Mizon, Grayham E and others},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Publisher = {Department of Economics, University of Oxford},
	Title = {Forecasting from structural econometric models},
	Year = {2012}}

@book{minsky:19882a,
	Author = {Minsky, Marvin and Papert, Seymour},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Publisher = {MIT Press, Cambridge, Ma},
	Title = {Perceptrons: an introduction to computational geometry (expanded edition)},
	Year = {1988}}

@book{rojas:20132a,
	Author = {Rojas, Ra{\'u}l},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Publisher = {Springer Science \& Business Media},
	Title = {Neural networks: a systematic introduction},
	Year = {2013}}

@article{rosenblatt:19582a,
	Author = {Rosenblatt, Frank},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Journal = {Psychological review},
	Number = {6},
	Pages = {386},
	Publisher = {American Psychological Association},
	Title = {The perceptron: A probabilistic model for information storage and organization in the brain.},
	Volume = {65},
	Year = {1958}}

@inproceedings{lucas:19762a,
	Author = {Lucas, Robert E},
	Booktitle = {Carnegie-Rochester conference series on public policy},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Organization = {Elsevier},
	Pages = {19--46},
	Title = {Econometric policy evaluation: A critique},
	Volume = {1},
	Year = {1976}}

@article{pescatori:20112a,
	Author = {Pescatori, Andrea and Zaman, Saeed and others},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Journal = {Economic Commentary},
	Number = {20},
	Pages = {1},
	Publisher = {Federal Reserve Bank of Cleveland},
	Title = {Macroeconomic models, forecasting, and policymaking},
	Volume = {19},
	Year = {2011}}

@article{sims:19802a,
	Author = {Sims, Christopher A},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Journal = {Econometrica: Journal of the Econometric Society},
	Pages = {1--48},
	Publisher = {JSTOR},
	Title = {Macroeconomics and reality},
	Year = {1980}}

@article{stark2013spf,
	Author = {Stark, Tom},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Journal = {Federal Reserve Bank of Philadelphia},
	Title = {SPF panelists' forecasting methods: A note on the aggregate results of a November 2009 special survey},
	Year = {2013}}

@article{rumelhart:19862a,
	Annote = {10.1038/323533a0},
	Author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	Date = {1986/10/09/print},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Day = {09},
	Journal = {Nature},
	M3 = {10.1038/323533a0},
	Month = {10},
	Number = {6088},
	Pages = {533--536},
	Title = {Learning representations by back-propagating errors},
	Ty = {JOUR},
	Url = {http://dx.doi.org/10.1038/323533a0},
	Volume = {323},
	Year = {1986},
	Bdsk-Url-1 = {http://dx.doi.org/10.1038/323533a0}}

@article{creel:20162a,
	Author = {Creel, Michael},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Journal = {Econometrics and Statistics},
	Publisher = {Elsevier},
	Title = {Neural Nets for Indirect Inference},
	Year = {2016}}

@article{gao:20162a,
	Abstract = {In this paper, we explore degrees of freedom in deep sigmoidal neural networks. We show that the degrees of freedom in these models is related to the expected optimism, which is the expected difference between test error and training error. We provide an efficient Monte-Carlo method to estimate the degrees of freedom for multi-class classification methods. We show degrees of freedom are lower than the parameter count in a simple XOR network. We extend these results to neural nets trained on synthetic and real data, and investigate impact of network's architecture and different regularization choices. The degrees of freedom in deep networks are dramatically smaller than the number of parameters, in some real datasets several orders of magnitude. Further, we observe that for fixed number of parameters, deeper networks have less degrees of freedom exhibiting a regularization-by-depth.},
	Author = {Tianxiang Gao and Vladimir Jojic},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Eprint = {1603.09260},
	Month = {03},
	Title = {Degrees of Freedom in Deep Neural Networks},
	Url = {https://arxiv.org/abs/1603.09260},
	Year = {2016},
	Bdsk-Url-1 = {https://arxiv.org/abs/1603.09260}}

@article{ye:19982a,
	Author = {Ye, Jianming},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Journal = {Journal of the American Statistical Association},
	Number = {441},
	Pages = {120--131},
	Publisher = {Taylor \& Francis},
	Title = {On measuring and correcting the effects of data mining and model selection},
	Volume = {93},
	Year = {1998}}

@article{cho:20142a,
	Abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
	Author = {Kyunghyun Cho and Bart van Merrienboer and Caglar Gulcehre and Dzmitry Bahdanau and Fethi Bougares and Holger Schwenk and Yoshua Bengio},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Eprint = {1406.1078},
	Month = {06},
	Title = {Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation},
	Url = {https://arxiv.org/abs/1406.1078},
	Year = {2014},
	Bdsk-Url-1 = {https://arxiv.org/abs/1406.1078}}

@article{sutskever:20142a,
	Abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
	Author = {Ilya Sutskever and Oriol Vinyals and Quoc V. Le},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Eprint = {1409.3215},
	Month = {09},
	Title = {Sequence to Sequence Learning with Neural Networks},
	Url = {https://arxiv.org/abs/1409.3215},
	Year = {2014},
	Bdsk-Url-1 = {https://arxiv.org/abs/1409.3215}}

@article{clements:20142a,
	Author = {Clements, Michael P},
	Date-Added = {2017-08-24 14:46:32 +0000},
	Date-Modified = {2017-08-24 14:46:32 +0000},
	Journal = {Journal of Business \& Economic Statistics},
	Number = {2},
	Pages = {206--216},
	Publisher = {Taylor \& Francis},
	Title = {Forecast uncertainty---ex ante and ex post: US inflation and output growth},
	Volume = {32},
	Year = {2014}}

@article{gal:20172a,
	Abstract = {Dropout is used as a practical tool to obtain uncertainty estimates in large vision models and reinforcement learning (RL) tasks. But to obtain well-calibrated uncertainty estimates, a grid-search over the dropout probabilities is necessary - a prohibitive operation with large models, and an impossible one with RL. We propose a new dropout variant which gives improved performance and better calibrated uncertainties. Relying on recent developments in Bayesian deep learning, we use a continuous relaxation of dropout's discrete masks. Together with a principled optimisation objective, this allows for automatic tuning of the dropout probability in large models, and as a result faster experimentation cycles. In RL this allows the agent to adapt its uncertainty dynamically as more data is observed. We analyse the proposed variant extensively on a range of tasks, and give insights into common practice in the field where larger dropout probabilities are often used in deeper model layers.},
	Author = {Yarin Gal and Jiri Hron and Alex Kendall},
	Date-Added = {2017-05-23 16:56:55 +0000},
	Date-Modified = {2017-05-23 16:57:03 +0000},
	Eprint = {1705.07832},
	Month = {05},
	Title = {Concrete Dropout},
	Url = {https://arxiv.org/abs/1705.07832},
	Year = {2017},
	Bdsk-Url-1 = {https://arxiv.org/abs/1705.07832}}

@article{zoph:20162a,
	Abstract = {Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.},
	Author = {Barret Zoph and Quoc V. Le},
	Date-Added = {2017-05-17 21:11:55 +0000},
	Date-Modified = {2017-05-17 21:12:02 +0000},
	Eprint = {1611.01578},
	Month = {11},
	Title = {Neural Architecture Search with Reinforcement Learning},
	Url = {https://arxiv.org/abs/1611.01578},
	Year = {2016},
	Bdsk-Url-1 = {https://arxiv.org/abs/1611.01578}}

@url{le:2a,
	Author = {Quoc Le AND Barret Zoph},
	Date-Added = {2017-05-17 21:08:13 +0000},
	Date-Modified = {2017-05-17 21:09:15 +0000},
	Title = {Using Machine Learning to Explore Neural Network Architecture},
	Url = {https://research.googleblog.com/2017/05/using-machine-learning-to-explore.html},
	Urldate = {05/17/2017},
	Bdsk-Url-1 = {https://research.googleblog.com/2017/05/using-machine-learning-to-explore.html}}

@url{husain:2a,
	Author = {Hamel Husain AND Nick Handel},
	Date-Added = {2017-05-17 21:06:32 +0000},
	Date-Modified = {2017-05-17 21:07:41 +0000},
	Title = {Automated Machine Learning --- A Paradigm Shift That Accelerates Data Scientist Productivity @ Airbnb},
	Url = {https://medium.com/airbnb-engineering/automated-machine-learning-a-paradigm-shift-that-accelerates-data-scientist-productivity-airbnb-f1f8a10d61f8},
	Urldate = {05/17/2017},
	Bdsk-Url-1 = {https://medium.com/airbnb-engineering/automated-machine-learning-a-paradigm-shift-that-accelerates-data-scientist-productivity-airbnb-f1f8a10d61f8}}

@article{gehring:20172a,
	Abstract = {The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations over all elements can be fully parallelized during training and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep LSTM setup of Wu et al. (2016) on both WMT'14 English-German and WMT'14 English-French translation at an order of magnitude faster speed, both on GPU and CPU.},
	Author = {Jonas Gehring and Michael Auli and David Grangier and Denis Yarats and Yann N. Dauphin},
	Date-Added = {2017-05-17 21:06:01 +0000},
	Date-Modified = {2017-05-17 21:06:01 +0000},
	Eprint = {1705.03122},
	Month = {05},
	Title = {Convolutional Sequence to Sequence Learning},
	Url = {https://arxiv.org/abs/1705.03122},
	Year = {2017},
	Bdsk-Url-1 = {https://arxiv.org/abs/1705.03122}}

@article{huang:20172a,
	Abstract = {Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters. To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective. We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively.},
	Author = {Gao Huang and Yixuan Li and Geoff Pleiss and Zhuang Liu and John E. Hopcroft and Kilian Q. Weinberger},
	Date-Added = {2017-05-17 21:04:49 +0000},
	Date-Modified = {2017-05-17 21:04:49 +0000},
	Eprint = {1704.00109},
	Month = {04},
	Title = {Snapshot Ensembles: Train 1, get M for free},
	Url = {https://arxiv.org/abs/1704.00109},
	Year = {2017},
	Bdsk-Url-1 = {https://arxiv.org/abs/1704.00109}}

@article{hansen:20172a,
	Abstract = {We present a new deep meta reinforcement learner, which we call Deep Episodic Value Iteration (DEVI). DEVI uses a deep neural network to learn a similarity metric for a non-parametric model-based reinforcement learning algorithm. Our model is trained end-to-end via back-propagation. Despite being trained using the model-free Q-learning objective, we show that DEVI's model-based internal structure provides `one-shot' transfer to changes in reward and transition structure, even for tasks with very high-dimensional state spaces.},
	Author = {Steven Stenberg Hansen},
	Date-Added = {2017-05-17 21:04:01 +0000},
	Date-Modified = {2017-05-17 21:04:08 +0000},
	Eprint = {1705.03562},
	Month = {05},
	Title = {Deep Episodic Value Iteration for Model-based Meta-Reinforcement Learning},
	Url = {https://arxiv.org/abs/1705.03562},
	Year = {2017},
	Bdsk-Url-1 = {https://arxiv.org/abs/1705.03562}}

@book{nielsen:20172a,
	Author = {Michael Nielsen},
	Date-Added = {2017-04-27 17:57:40 +0000},
	Date-Modified = {2017-04-27 17:58:08 +0000},
	Publisher = {web},
	Title = {Neural Networks and Deep Learning},
	Url = {http://neuralnetworksanddeeplearning.com/index.html},
	Year = {2017},
	Bdsk-Url-1 = {http://neuralnetworksanddeeplearning.com/index.html}}

@article{li:20172b,
	Abstract = {To obtain uncertainty estimates with real-world Bayesian deep learning models, practical inference approximations are needed. Dropout variational inference (VI) for example has been used for machine vision and medical applications, but VI can severely underestimates model uncertainty. Alpha-divergences are alternative divergences to VI's KL objective, which are able to avoid VI's uncertainty underestimation. But these are hard to use in practice: existing techniques can only use Gaussian approximating distributions, and require existing models to be changed radically, thus are of limited use for practitioners. We propose a re-parametrisation of the alpha-divergence objectives, deriving a simple inference technique which, together with dropout, can be easily implemented with existing models by simply changing the loss of the model. We demonstrate improved uncertainty estimates and accuracy compared to VI in dropout networks. We study our model's epistemic uncertainty far away from the data using adversarial images, showing that these can be distinguished from non-adversarial images by examining our model's uncertainty.},
	Author = {Yingzhen Li and Yarin Gal},
	Date-Added = {2017-04-27 17:56:38 +0000},
	Date-Modified = {2017-04-27 17:56:38 +0000},
	Eprint = {1703.02914},
	Month = {03},
	Title = {Dropout Inference in Bayesian Neural Networks with Alpha-divergences},
	Url = {https://arxiv.org/abs/1703.02914},
	Year = {2017},
	Bdsk-Url-1 = {https://arxiv.org/abs/1703.02914}}

@article{li:20172a,
	Abstract = {We give an overview of recent exciting achievements of deep reinforcement learning (RL). We start with background of deep learning and reinforcement learning, as well as introduction of testbeds. Next we discuss Deep Q-Network (DQN) and its extensions, asynchronous methods, policy optimization, reward, and planning. After that, we talk about attention and memory, unsupervised learning, and learning to learn. Then we discuss various applications of RL, including games, in particular, AlphaGo, robotics, spoken dialogue systems (a.k.a. chatbot), machine translation, text sequence prediction, neural architecture design, personalized web services, healthcare, finance, and music generation. We mention topics/papers not reviewed yet. After listing a collection of RL resources, we close with discussions.},
	Author = {Yuxi Li},
	Date-Added = {2017-04-27 17:55:47 +0000},
	Date-Modified = {2017-04-27 17:55:47 +0000},
	Eprint = {1701.07274},
	Month = {01},
	Title = {Deep Reinforcement Learning: An Overview},
	Url = {https://arxiv.org/abs/1701.07274},
	Year = {2017},
	Bdsk-Url-1 = {https://arxiv.org/abs/1701.07274}}

@book{rasmussen:20062a,
	Author = {Carl Edward Rasmussen and Christopher K. I. Williams},
	Date-Added = {2017-04-27 17:53:50 +0000},
	Date-Modified = {2017-04-27 17:54:39 +0000},
	Publisher = {MIT Press},
	Title = {Gaussian Processes for Machine Learning},
	Url = {http://www.gaussianprocess.org/gpml/chapters/},
	Year = {2006},
	Bdsk-Url-1 = {http://www.gaussianprocess.org/gpml/chapters/}}

@electronic{olah:2a,
	Author = {Christopher Olah},
	Date-Added = {2017-04-27 17:52:22 +0000},
	Date-Modified = {2017-04-27 17:53:42 +0000},
	Title = {Understanding LSTM Networks},
	Url = {http://colah.github.io/posts/2015-08-Understanding-LSTMs/},
	Urldate = {2015},
	Bdsk-Url-1 = {http://colah.github.io/posts/2015-08-Understanding-LSTMs/}}

@electronic{karpathy:2a,
	Author = {Andrej Karpathy},
	Date-Added = {2017-04-27 17:51:34 +0000},
	Date-Modified = {2017-04-27 17:52:08 +0000},
	Title = {The Unreasonable Effectiveness of Recurrent Neural Networks},
	Url = {http://karpathy.github.io/2015/05/21/rnn-effectiveness/},
	Urldate = {May 21, 2015},
	Bdsk-Url-1 = {http://karpathy.github.io/2015/05/21/rnn-effectiveness/}}

@article{qi:20082a,
	Author = {Qi, Min and Zhang, G Peter},
	Date-Added = {2017-04-27 17:50:50 +0000},
	Date-Modified = {2017-04-27 17:50:51 +0000},
	Journal = {IEEE Transactions on neural networks},
	Number = {5},
	Pages = {808--816},
	Publisher = {IEEE},
	Title = {Trend time--series modeling and forecasting with neural networks},
	Volume = {19},
	Year = {2008}}

@electronic{ruder:2a,
	Author = {Sebastian Ruder},
	Date-Added = {2017-04-27 17:49:27 +0000},
	Date-Modified = {2017-04-27 17:50:10 +0000},
	Title = {An overview of gradient descent optimization algorithms},
	Url = {http://sebastianruder.com/optimizing-gradient-descent/},
	Urldate = {2017},
	Bdsk-Url-1 = {http://sebastianruder.com/optimizing-gradient-descent/}}

@electronic{koller:2a,
	Annote = {https://www.youtube.com/playlist?list=PL50E6E80E8525B59C
https://www.coursera.org/learn/probabilistic-graphical-models
},
	Author = {Daphne Koller},
	Date-Added = {2017-04-27 17:44:17 +0000},
	Date-Modified = {2017-04-27 17:48:55 +0000},
	Howpublished = {web course},
	Title = {probabilistic graph models},
	Url = {http://openclassroom.stanford.edu/MainFolder/CoursePage.php?course=ProbabilisticGraphicalModels},
	Urldate = {2017},
	Bdsk-Url-1 = {http://openclassroom.stanford.edu/MainFolder/CoursePage.php?course=ProbabilisticGraphicalModels}}

@article{nielsen:20152a,
	Author = {Nielsen, Michael A},
	Date-Added = {2017-03-20 14:33:15 +0000},
	Date-Modified = {2017-03-20 14:33:18 +0000},
	Journal = {URL: http://neuralnetworksanddeeplearning. com/.(visited: 01.11. 2014)},
	Title = {Neural networks and deep learning},
	Year = {2015}}

@inproceedings{gal:20152b,
	Author = {Gal, Yarin and Ghahramani, Zoubin},
	Booktitle = {Deep Learning Workshop, ICML},
	Date-Added = {2017-03-14 19:03:26 +0000},
	Date-Modified = {2017-03-14 19:04:27 +0000},
	Title = {Dropout as a Bayesian approximation: Insights and applications},
	Url = {http://ece.duke.edu/~lcarin/Chunyuan1.15.2016.pdf , https://arxiv.org/abs/1506.02157},
	Year = {2015},
	Bdsk-Url-1 = {http://ece.duke.edu/~lcarin/Chunyuan1.15.2016.pdf%20,%20https://arxiv.org/abs/1506.02157}}

@article{gal:20152a,
	Abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
	Author = {Yarin Gal and Zoubin Ghahramani},
	Date-Added = {2017-03-14 19:00:02 +0000},
	Date-Modified = {2017-03-14 19:00:02 +0000},
	Eprint = {1506.02142},
	Month = {06},
	Title = {Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning},
	Url = {https://arxiv.org/abs/1506.02142},
	Year = {2015},
	Bdsk-Url-1 = {https://arxiv.org/abs/1506.02142}}

@phdthesis{Gal2016Uncertainty,
	Author = {Gal, Yarin},
	Date-Added = {2017-03-02 19:28:32 +0000},
	Date-Modified = {2017-03-02 19:28:32 +0000},
	School = {University of Cambridge},
	Title = {Uncertainty in Deep Learning},
	Year = {2016}}

@article{zhou:20172a,
	Abstract = {In this paper, we propose gcForest, a decision tree ensemble approach with performance highly competitive to deep neural networks. In contrast to deep neural networks which require great effort in hyper-parameter tuning, gcForest is much easier to train. Actually, even when gcForest is applied to different data from different domains, excellent performance can be achieved by almost same settings of hyper-parameters. The training process of gcForest is efficient and scalable. In our experiments its training time running on a PC is comparable to that of deep neural networks running with GPU facilities, and the efficiency advantage may be more apparent because gcForest is naturally apt to parallel implementation. Furthermore, in contrast to deep neural networks which require large-scale training data, gcForest can work well even when there are only small-scale training data. Moreover, as a tree-based approach, gcForest should be easier for theoretical analysis than deep neural networks.},
	Author = {Zhi-Hua Zhou and Ji Feng},
	Date-Added = {2017-03-02 19:27:59 +0000},
	Date-Modified = {2017-03-02 19:27:59 +0000},
	Eprint = {1702.08835},
	Month = {02},
	Title = {Deep Forest: Towards An Alternative to Deep Neural Networks},
	Url = {https://arxiv.org/abs/1702.08835},
	Year = {2017},
	Bdsk-Url-1 = {https://arxiv.org/abs/1702.08835}}

@article{wang:20172a,
	Abstract = {This paper is a review of the evolutionary history of deep learning models. It covers from the genesis of neural networks when associationism modeling of the brain is studied, to the models that dominate the last decade of research in deep learning like convolutional neural networks, deep belief networks, and recurrent neural networks, and extends to popular recent models like variational autoencoder and generative adversarial nets. In addition to a review of these models, this paper primarily focuses on the precedents of the models above, examining how the initial ideas are assembled to construct the early models and how these preliminary models are developed into their current forms. Many of these evolutionary paths last more than half a century and have a diversity of directions. For example, CNN is built on prior knowledge of biological vision system; DBN is evolved from a trade-off of modeling power and computation complexity of graphical models and many nowadays models are neural counterparts of ancient linear models. This paper reviews these evolutionary paths and offers a concise thought flow of how these models are developed, and aims to provide a thorough background for deep learning. More importantly, along with the path, this paper summarizes the gist behind these milestones and proposes many directions to guide the future research of deep learning.},
	Author = {Haohan Wang and Bhiksha Raj},
	Date-Added = {2017-03-02 19:27:42 +0000},
	Date-Modified = {2017-03-02 19:27:42 +0000},
	Eprint = {1702.07800},
	Month = {02},
	Title = {On the Origin of Deep Learning},
	Url = {https://arxiv.org/abs/1702.07800},
	Year = {2017},
	Bdsk-Url-1 = {https://arxiv.org/abs/1702.07800}}

@article{parisotto:20172a,
	Abstract = {A critical component to enabling intelligent reasoning in partially observable environments is memory. Despite this importance, Deep Reinforcement Learning (DRL) agents have so far used relatively simple memory architectures, with the main methods to overcome partial observability being either a temporal convolution over the past k frames or an LSTM layer. More recent work (Oh et al., 2016) has went beyond these architectures by using memory networks which can allow more sophisticated addressing schemes over the past k frames. But even these architectures are unsatisfactory due to the reason that they are limited to only remembering information from the last k frames. In this paper, we develop a memory system with an adaptable write operator that is customized to the sorts of 3D environments that DRL agents typically interact with. This architecture, called the Neural Map, uses a spatially structured 2D memory image to learn to store arbitrary information about the environment over long time lags. We demonstrate empirically that the Neural Map surpasses previous DRL memories on a set of challenging 2D and 3D maze environments and show that it is capable of generalizing to environments that were not seen during training.},
	Author = {Emilio Parisotto and Ruslan Salakhutdinov},
	Date-Added = {2017-03-02 19:27:34 +0000},
	Date-Modified = {2017-03-02 19:27:34 +0000},
	Eprint = {1702.08360},
	Month = {02},
	Title = {Neural Map: Structured Memory for Deep Reinforcement Learning},
	Url = {https://arxiv.org/abs/1702.08360},
	Year = {2017},
	Bdsk-Url-1 = {https://arxiv.org/abs/1702.08360}}

@article{mao:20162a,
	Abstract = {Unsupervised learning with generative adversarial networks (GANs) has proven hugely successful. Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function. This loss function, however, may lead to the vanishing gradient problem during the learning process. To overcome such problem, here we propose the Least Squares Generative Adversarial Networks (LSGANs) that adopt the least squares loss function for the discriminator. We show that minimizing the objective function of LSGAN yields minimizing the Pearson $\chi^2$ divergence. There are two benefits of LSGANs over regular GANs. First, LSGANs are able to generate higher quality images than regular GANs. Second, LSGANs performs more stable during the learning process. We evaluate the LSGANs on five scene datasets and the experimental results demonstrate that the generated images by LSGANs look more realistic than the ones generated by regular GANs. We also conduct two comparison experiments between LSGANs and regular GANs to illustrate the stability of LSGANs.},
	Author = {Xudong Mao and Qing Li and Haoran Xie and Raymond Y.K. Lau and Zhen Wang},
	Date-Added = {2017-03-02 19:27:27 +0000},
	Date-Modified = {2017-03-02 19:27:27 +0000},
	Eprint = {1611.04076},
	Month = {11},
	Title = {Least Squares Generative Adversarial Networks},
	Url = {https://arxiv.org/abs/1611.04076},
	Year = {2016},
	Bdsk-Url-1 = {https://arxiv.org/abs/1611.04076}}

@article{nachum:20172a,
	Abstract = {We formulate a new notion of softmax temporal consistency that generalizes the standard hard-max Bellman consistency usually considered in value based reinforcement learning (RL). In particular, we show how softmax consistent action values correspond to optimal policies that maximize entropy regularized expected reward. More importantly, we establish that softmax consistent action values and the optimal policy must satisfy a mutual compatibility property that holds across any state-action subsequence. Based on this observation, we develop a new RL algorithm, Path Consistency Learning (PCL), that minimizes the total inconsistency measured along multi-step subsequences extracted from both both on and off policy traces. An experimental evaluation demonstrates that PCL significantly outperforms strong actor-critic and Q-learning baselines across several benchmark tasks.},
	Author = {Ofir Nachum and Mohammad Norouzi and Kelvin Xu and Dale Schuurmans},
	Date-Added = {2017-03-02 19:27:04 +0000},
	Date-Modified = {2017-03-02 19:27:04 +0000},
	Eprint = {1702.08892},
	Month = {02},
	Title = {Bridging the Gap Between Value and Policy Based Reinforcement Learning},
	Url = {https://arxiv.org/abs/1702.08892},
	Year = {2017},
	Bdsk-Url-1 = {https://arxiv.org/abs/1702.08892}}

@article{amos:20172a,
	Abstract = {This paper presents OptNet, a network architecture that integrates optimization problems (here, specifically in the form of quadratic programs) as individual layers in larger end-to-end trainable deep networks. These layers allow complex dependencies between the hidden states to be captured that traditional convolutional and fully-connected layers are not able to capture. In this paper, we develop the foundations for such an architecture: we derive the equations to perform exact differentiation through these layers and with respect to layer parameters; we develop a highly efficient solver for these layers that exploits fast GPU-based batch solves within a primal-dual interior point method, and which provides backpropagation gradients with virtually no additional cost on top of the solve; and we highlight the application of these approaches in several problems. In one particularly standout example, we show that the method is capable of learning to play Sudoku given just input and output games, with no a priori information about the rules of the game; this task is virtually impossible for other neural network architectures that we have experimented with, and highlights the representation capabilities of our approach.},
	Author = {Brandon Amos and J. Zico Kolter},
	Date-Added = {2017-03-02 19:26:45 +0000},
	Date-Modified = {2017-03-02 19:26:45 +0000},
	Eprint = {1703.00443},
	Month = {03},
	Title = {OptNet: Differentiable Optimization as a Layer in Neural Networks},
	Url = {https://arxiv.org/abs/1703.00443},
	Year = {2017},
	Bdsk-Url-1 = {https://arxiv.org/abs/1703.00443}}

@article{kaiser:20172a,
	Author = {Lukasz Kaiser AND Ofir Nachum AND Aurko Roy AND Samy Bengio},
	Date-Added = {2017-01-06 16:44:57 +0000},
	Date-Modified = {2017-01-06 16:44:57 +0000},
	Journal = {ICLR},
	Title = {Learning to Remember Rare Events},
	Url = {https://openreview.net/pdf?id=SJTQLdqlg},
	Year = {2017},
	Bdsk-Url-1 = {https://openreview.net/pdf?id=SJTQLdqlg}}

@electronic{sung:20172a,
	Author = {Flood Sung},
	Date-Added = {2017-01-04 15:25:06 +0000},
	Date-Modified = {2017-01-04 15:26:49 +0000},
	Lastchecked = {01/04/2017},
	Month = {01},
	Url = {https://github.com/songrotek/Deep-Learning-Papers-Reading-Roadmap},
	Year = {2017},
	Bdsk-Url-1 = {https://github.com/songrotek/Deep-Learning-Papers-Reading-Roadmap}}

@article{hartford:20162c,
	Abstract = {We are in the middle of a remarkable rise in the use and capability of artificial intelligence. Much of this growth has been fueled by the success of deep learning architectures: models that map from observables to outputs via multiple layers of latent representations. These deep learning algorithms are effective tools for unstructured prediction, and they can be combined in AI systems to solve complex automated reasoning problems. This paper provides a recipe for combining ML algorithms to solve for causal effects in the presence of instrumental variables -- sources of treatment randomization that are conditionally independent from the response. We show that a flexible IV specification resolves into two prediction tasks that can be solved with deep neural nets: a first-stage network for treatment prediction and a second-stage network whose loss function involves integration over the conditional treatment distribution. This Deep IV framework imposes some specific structure on the stochastic gradient descent routine used for training, but it is general enough that we can take advantage of off-the-shelf ML capabilities and avoid extensive algorithm customization. We outline how to obtain out-of-sample causal validation in order to avoid over-fit. We also introduce schemes for both Bayesian and frequentist inference: the former via a novel adaptation of dropout training, and the latter via a data splitting routine.},
	Author = {Jason Hartford and Greg Lewis and Kevin Leyton-Brown and Matt Taddy},
	Date-Added = {2017-01-04 14:54:22 +0000},
	Date-Modified = {2017-01-04 14:54:22 +0000},
	Eprint = {1612.09596},
	Month = {12},
	Title = {Counterfactual Prediction with Deep Instrumental Variables Networks},
	Url = {https://arxiv.org/abs/1612.09596},
	Year = {2016},
	Bdsk-Url-1 = {https://arxiv.org/abs/1612.09596}}

@article{webb1999two,
	Author = {Webb, Roy H and others},
	Date-Added = {2017-01-03 20:32:06 +0000},
	Date-Modified = {2017-01-03 20:32:06 +0000},
	Journal = {Economic Quarterly-Federal Reserve Bank of Richmond},
	Number = {3},
	Pages = {23--40},
	Publisher = {THE FEDERAL RESERVE BANK OF RICHMOND},
	Title = {Two approaches to macroeconomic forecasting},
	Volume = {85},
	Year = {1999}}

@article{cheng:20162a,
	Abstract = {Generalized linear models with nonlinear feature transformations are widely used for large-scale regression and classification problems with sparse inputs. Memorization of feature interactions through a wide set of cross-product feature transformations are effective and interpretable, while generalization requires more feature engineering effort. With less feature engineering, deep neural networks can generalize better to unseen feature combinations through low-dimensional dense embeddings learned for the sparse features. However, deep neural networks with embeddings can over-generalize and recommend less relevant items when the user-item interactions are sparse and high-rank. In this paper, we present Wide & Deep learning---jointly trained wide linear models and deep neural networks---to combine the benefits of memorization and generalization for recommender systems. We productionized and evaluated the system on Google Play, a commercial mobile app store with over one billion active users and over one million apps. Online experiment results show that Wide & Deep significantly increased app acquisitions compared with wide-only and deep-only models. We have also open-sourced our implementation in TensorFlow.},
	Author = {Heng-Tze Cheng and Levent Koc and Jeremiah Harmsen and Tal Shaked and Tushar Chandra and Hrishi Aradhye and Glen Anderson and Greg Corrado and Wei Chai and Mustafa Ispir and Rohan Anil and Zakaria Haque and Lichan Hong and Vihan Jain and Xiaobing Liu and Hemal Shah},
	Date-Added = {2016-10-19 19:04:24 +0000},
	Date-Modified = {2016-10-19 19:04:24 +0000},
	Eprint = {1606.07792},
	Month = {06},
	Title = {Wide & Deep Learning for Recommender Systems},
	Url = {https://arxiv.org/abs/1606.07792},
	Year = {2016},
	Bdsk-Url-1 = {https://arxiv.org/abs/1606.07792}}

@webpage{Department:aa,
	Author = {Computer Science Department, Stanford University.},
	Date-Added = {2016-09-27 21:01:27 +0000},
	Date-Modified = {2017-03-02 19:23:33 +0000},
	Title = {Deep Learning},
	Url = {http://ufldl.stanford.edu/},
	Bdsk-Url-1 = {http://ufldl.stanford.edu/}}

@article{kriesel2007brief,
	Author = {Kriesel, David},
	Date-Added = {2016-09-27 16:48:13 +0000},
	Date-Modified = {2016-09-27 21:03:31 +0000},
	Publisher = {Citeseer},
	Title = {A brief Introduction on Neural Networks},
	Url = {http://www.dkriesel.com/_media/science/neuronalenetze-en-zeta2-2col-dkrieselcom.pdf},
	Year = {2007},
	Bdsk-Url-1 = {http://www.dkriesel.com/_media/science/neuronalenetze-en-zeta2-2col-dkrieselcom.pdf}}
